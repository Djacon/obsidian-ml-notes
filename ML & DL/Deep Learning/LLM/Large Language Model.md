Large Language Model (LLM) is a type of [[algorithms|algorithm]] in [[deep learning]] that applies [[neural network]] with lots of parameters to process and understand human languages or text using self-supervised learning techniques

### LLM Roadmap:

#### 1. LLM Architecture:

- Tokenization
- Attention mechanisms
- Text generation
#### 2. Building a intruction dataset:

- Alpaka-like dataset
- Filtering data
- Prompt templates
#### 3. Pre-training models:

- Data pipeline
- Causal language modeling
- Scaling laws
#### 4. Supervised Fine-tuning:

* Fine-tuning
* LoRA and QLoRA
* Axolotl
* DeepSpeed

#### 5. RLHF
#### 6. Evaluation:

- Traditional metrics
- General benchmarks
- Task-specific benchmarks
- Human evaluation

#### 7. Quantization:

* Base techniques
* GGUF and llama.cpp
* GPTQ and EXL2
* AWQ

#### 8. New Trends:

* Positional embeddings
* Model merging
* Mixture of Experts (MoE)
* Multimodal models