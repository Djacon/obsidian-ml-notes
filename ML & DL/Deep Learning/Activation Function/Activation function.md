**Activation function** is a [[math]] “gate” in between the input feeding the current neuron and its output going to the next layer. They basically decide whether the neuron should be activated or not

#### Types of Activation functions:

* [[Sigmoid]]
* Hyperbolid Tangent (Tanh)
* Rectified Linear Unit (ReLU)
* Leaky ReLU

![[Activator_image.png]]