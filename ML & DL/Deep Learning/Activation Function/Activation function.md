**Activation function** is a [[math]] function that is applied to the output of a neuron in a [[neural networks]]. It introduces non-linearity to the output of the neuron, allowing the neural network to learn and [[model]] complex relationships between input features and output targets

#### Types of Activation functions:

* [[Sigmoid]]
* [[Hyperbolid Tangent]] (Tanh)
* [[Rectified Linear Unit]] (ReLU)
* [[Leaky ReLU]]

![[Activator_image.png]]