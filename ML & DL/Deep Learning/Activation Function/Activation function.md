**Activation function** is a [[math]] function that is applied to the output of a [[neuron]] in a [[neural network|neural networks]]. It introduces non-linearity to the output of the neuron, allowing the neural network to learn and [[model]] complex relationships between input features and output targets

#### Types of Activation functions:

* [[Sigmoid]]
* [[Hyperbolid Tangent|Tanh]] (Hyperbolid Tangent)
* [[Rectified Linear Unit||ReLU]] (Rectified Linear Unit)
* [[Leaky ReLU]]
* [[Maxout]]

![[Activator_image.png]]